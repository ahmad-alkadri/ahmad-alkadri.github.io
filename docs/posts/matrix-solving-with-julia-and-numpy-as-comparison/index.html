<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Matrix Solving with Julia (and Numpy as comparison) | Ahmad Alkadri</title>
<meta name="keywords" content="">
<meta name="description" content="In which I got curious, rekindled my past with Julia (the language), and did some benchmark tests against NumPy.">
<meta name="author" content="">
<link rel="canonical" href="/posts/matrix-solving-with-julia-and-numpy-as-comparison/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ce308c09b4317de5915a254af8d25c3549129148eb8584d37d76782e901c8f27.css" integrity="sha256-zjCMCbQxfeWRWiVK&#43;NJcNUkSkUjrhYTTfXZ4LpAcjyc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Matrix Solving with Julia (and Numpy as comparison)" />
<meta property="og:description" content="In which I got curious, rekindled my past with Julia (the language), and did some benchmark tests against NumPy." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/matrix-solving-with-julia-and-numpy-as-comparison/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-11T04:29:25+01:00" />
<meta property="article:modified_time" content="2023-10-11T04:29:25+01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Matrix Solving with Julia (and Numpy as comparison)"/>
<meta name="twitter:description" content="In which I got curious, rekindled my past with Julia (the language), and did some benchmark tests against NumPy."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "/posts/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Matrix Solving with Julia (and Numpy as comparison)",
      "item": "/posts/matrix-solving-with-julia-and-numpy-as-comparison/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Matrix Solving with Julia (and Numpy as comparison)",
  "name": "Matrix Solving with Julia (and Numpy as comparison)",
  "description": "In which I got curious, rekindled my past with Julia (the language), and did some benchmark tests against NumPy.",
  "keywords": [
    
  ],
  "articleBody": "A lot of things happened between my last post and now. Family stuff, work problems, dealing with bureaucracy—the list keeps growing.\nBut there are some good things too. One of them being that I finally beat my procrastination and finished Busuu’s courses for B1 level, and I even got the official certificate! You can check it out here. The kanji and listening parts were a bit tougher than I expected, lol.\nApart from that, I’ve been busy with a bunch of personal projects and experiments. One of them involves Julia—a high-level, dynamic language made for doing computation stuff. I first came across it during my PhD in France years ago.\nBack then, I was looking for a different language, framework, or platform to do numerical simulations. I’m talking about stuff like finite element, finite difference, and solving big matrices with tons and tons of elements over time (unsteady state). I needed something fast, easy to use and code, and easy for other people to use too.\nJulia was one of the options I found. I was amazed at how easy it was to install and set up. A couple of minutes and I was ready to go, writing my first functions.\n# hello.jl function say_hello() ## It looked more or less like this println(\"Hello, world!\") end say_hello() The language seemed promising and performed well in tests, but I ultimately chose not to use it. Instead, I considered alternatives like MATLAB and Cast3M, which my supervisors were already familiar with.\nHowever, things have changed now.\nFrom Academia to Industry Lots of things happened, but basically I transitioned from academia to industry. Over the course of (almost) four years, I’ve gained more skills and experiences in software engineering than ever before. Building REST APIs, full-stack web apps, scrapers, and many more tools that are used on a daily basis at my company.\nThese days, though, when building a calculation tool, I came to realize a new requirement: speed. This is especially important when calling remote functions. Nobody wants to wait 10 seconds just to finish a single calculation made by changing a single parameter. That’s why I’ve been contemplating seriously building my own finite element solver instead of relying on my usual tools.\nIt was here that Julia re-entered my radar. Originally thinking to just stay in my comfort zone with Python and its critically acclaimed numerical package, NumPy, I decided to test it out against Julia. Although that wasn’t the main reason for me to see Julia again—it was actually Julia’s threading and distributed computing capability—but we’ll talk about that in another post.\nAfter all, NumPy is already well-known amongst the scientific computation community as having one of the easiest to use, and fastest matrix solver out there. Why not benchmark it against Julia, who’s been claimed to be as fast Fortran, with ease of use as Python?\nDiving to the Code Matrix Generation The first thing I did was preparing the structure. I was thinking that the project needs at least three files: one code file to generate the matrices, one file to benchmark NumPy’s matrix solver, and one file to benchmark Julia’s matrix solver.\nThus, I prepared a simple project structure like this:\n├── julia_side.jl ├── matrix_generation.py └── numpy_side.py I decided to use NumPy’s own matrix generation capability because, why not?\nSo firstly, the inside of the matrix_generation.py is as follows:\nimport numpy as np # Define matrix sizes matrix_sizes = [10, 100, 500, 1000, 1500, 2000] # Set the seed for reproducibility np.random.seed(0) # Define the range for matrix elements low_value = -10 high_value = 10 for matrix_size in matrix_sizes: # Generate matrices A = np.random.uniform(low_value, high_value, (matrix_size, matrix_size)) x = np.random.uniform(low_value, high_value, matrix_size) # Calculate B such that Ax = B B = np.dot(A, x) # Save matrices with size info np.savetxt(f\"matrix_A_{matrix_size}.txt\", A) np.savetxt(f\"vector_B_{matrix_size}.txt\", B) As you can see, I created six A matrices and six B matrices with varying dimensions. To make sure that a matrix x solution exists, I generated x first and then generated matrices B through the dot operation of A and x. Then, I saved each of those matrices in simple text files.\nOh, also, I set the seed in NumPy, to make sure that the whole thing is reproducible.\nBenchmark NumPy Next is writing a code to do benchmark of NumPy’s matrix solver. To do this, I simply use two modules: memory_profiler and timeit to measure both the execution time and memory usage.\nfrom memory_profiler import memory_usage import numpy as np import timeit import gc matrix_sizes = [10, 100, 500, 1000, 1500, 2000] def solve_equation(A, B): return np.linalg.solve(A, B) results = [] for matrix_size in matrix_sizes: # Load matrices matrix_A = np.loadtxt(f\"matrix_A_{matrix_size}.txt\") vector_B = np.loadtxt(f\"vector_B_{matrix_size}.txt\") # Time the function using timeit elapsed_time = timeit.timeit( \"solve_equation(matrix_A, vector_B)\", setup=f\"from __main__ import solve_equation, matrix_A, vector_B\", number=1, # Number of executions ) # Measure memory mem_usage_before = memory_usage(max_usage=True) mem_usage, vector_x = memory_usage( (solve_equation, (matrix_A, vector_B)), max_usage=True, retval=True ) mem_usage_increment = mem_usage - mem_usage_before # Append results results.append((matrix_size, elapsed_time, mem_usage_increment)) # Cleanup del matrix_A, vector_B, vector_x gc.collect() # Save results to CSV np.savetxt( \"numpy_results.csv\", results, delimiter=\",\", header=\"MatrixSize,Time,Memory\", comments=\"\", fmt=[\"%d\", \"%.6f\", \"%.6f\"], ) The results are then saved as a CSV file.This is important because I want to be able to compare the data easily later.\nBenchmark Julia Next step is writing a Julia code to benchmark its solver. In Julia language, there’s a package called BenchmarkTools which is very practical for measuring the memory and time processing at the same time.\nusing BenchmarkTools, CSV, DataFrames, DelimitedFiles matrix_sizes = [10, 100, 500, 1000, 1500, 2000] results = [] function solve_equation(A, B) x = A \\\\ B end for matrix_size in matrix_sizes # Use let block to limit the scope of variables and help with memory management let matrix_A = Matrix(CSV.File(\"matrix_A_$(matrix_size).txt\"; header = false) |\u003e DataFrame), df_B = CSV.File(\"vector_B_$(matrix_size).txt\"; header = false) |\u003e DataFrame, vector_B = df_B[!, 1] # Extract the first column without copying # Run the function and benchmark bm = @benchmark solve_equation($matrix_A, $vector_B) push!(results, (matrix_size, mean(bm).time / 1e9, mean(bm).memory / 1e6)) # Explicitly call the garbage collector GC.gc() end end # Save results to CSV open(\"julia_results.csv\", \"w\") do io println(io, \"MatrixSize,Time,Memory\") # Adding a header line writedlm(io, results, ',') # Writing data below the header end Additionally, I also use DataFrame and CSV packages for help in reading the matrices which are saved as text files.\nThen, just like the benchmark code using NumPy, I also export the result as CSV files.\nResults I ran the tests using my own machine, whose specs can be summed as:\nFedora Linux 38 64-bit 16 GiB System Memory 11th Gen Intel® Core™ i5-11400H × 12 Processors And below is the table with the results:\nMatrixSize NumPyTime NumPyMemory JuliaTime JuliaMemory 10 7.60e-4 0.250 1.12e-6 0.001 100 3.95e-4 0.125 1.52e-4 0.082 500 4.05e-3 1.500 2.17e-3 2.008 1000 2.51e-2 4.625 7.79e-3 8.016 1500 2.61e-2 9.500 1.90e-2 18.024 2000 9.78e-2 13.375 3.86e-2 32.032 From the data, it can be observed that Julia generally performs faster than NumPy for solving matrices. In smaller matrices, the difference can be between two or three times higher; however, with the increase of matrix dimensions, the time difference became lower and lower. This makes me curious if the time difference will at some point disappear. I’m going to study this part later.\nCuriously, though, NumPy tends to use less memory compared to Julia. The performance difference becomes more prominent as the size of the matrices increases. We’re seeing about two to almost three times differences at the end. This could also be a big consideration point.\nNext Steps The benchmark tests above have shown that Julia consistently performs faster than NumPy for solving matrices. The next steps of the tests or decisions to make, in my opinion, would be:\nTest for even bigger matrices (to-do next: 3000, 4000, 5000,…. the list goes on). Try other ways to input the data, such as REST API calls, instead of reading from a text file. Test in real-time use, such as setting them up as serverless functions to be called or using REST API calls with workers and queue management. The whole code used for the benchmark test above has been uploaded to a GitHub repository. You’re free to test it out, adjust, and modify. I’ll probably also add more test cases (especially for bigger matrices) in the near future, so stay in touch!\nAlso, don’t hesitate to raise any questions or comments below!\n",
  "wordCount" : "1404",
  "inLanguage": "en",
  "datePublished": "2023-10-11T04:29:25+01:00",
  "dateModified": "2023-10-11T04:29:25+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/posts/matrix-solving-with-julia-and-numpy-as-comparison/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ahmad Alkadri",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="Ahmad Alkadri (Alt + H)">Ahmad Alkadri</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="/page/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Matrix Solving with Julia (and Numpy as comparison)
    </h1>
    <div class="post-description">
      In which I got curious, rekindled my past with Julia (the language), and did some benchmark tests against NumPy.
    </div>
    <div class="post-meta"><span title='2023-10-11 04:29:25 +0100 +0100'>Wed, Oct 11, 2023</span>

</div>
  </header> 
  <div class="post-content"><p>A lot of things happened between my last post and now. Family stuff, work
problems, dealing with bureaucracy—the list keeps growing.</p>
<p>But there are some good things too. One of them being that I finally beat my
procrastination and finished Busuu&rsquo;s courses for B1 level, and I even got the
official certificate! You can check it out
<a href="https://www.linkedin.com/feed/update/urn:li:activity:7114248182447894528/">here</a>.
The kanji and listening parts were a bit tougher than I expected, lol.</p>
<p>Apart from that, I&rsquo;ve been busy with a bunch of personal projects and
experiments. One of them involves Julia—a high-level, dynamic language made for
doing computation stuff. I first came across it during my PhD in France years
ago.</p>
<p>Back then, I was looking for a different language, framework, or platform to do
numerical simulations. I&rsquo;m talking about stuff like finite element, finite
difference, and solving big matrices with tons and tons of elements over time
(unsteady state). I needed something fast, easy to use and code, and easy for
other people to use too.</p>
<p>Julia was one of the options I found. I was amazed at how easy it was to install
and set up. A couple of minutes and I was ready to go, writing my first
functions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#75715e"># hello.jl</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">function</span> say_hello()
</span></span><span style="display:flex;"><span><span style="color:#75715e">## It looked more or less like this</span>
</span></span><span style="display:flex;"><span>    println(<span style="color:#e6db74">&#34;Hello, world!&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>say_hello() 
</span></span></code></pre></div><p>The language seemed promising and performed well in tests, but I ultimately
chose not to use it. Instead, I considered alternatives like
<a href="https://www.mathworks.com/products/pde.html">MATLAB</a> and
<a href="http://www-cast3m.cea.fr/">Cast3M</a>, which my supervisors were already familiar
with.</p>
<p>However, things have changed now.</p>
<h2 id="from-academia-to-industry">From Academia to Industry<a hidden class="anchor" aria-hidden="true" href="#from-academia-to-industry">#</a></h2>
<p>Lots of things happened, but basically I transitioned from academia to industry.
Over the course of (almost) four years, I’ve gained more skills and experiences
in software engineering than ever before. Building REST APIs, full-stack web
apps, scrapers, and many more tools that are used on a daily basis at my
company.</p>
<p>These days, though, when building a calculation tool, I came to realize a new
requirement: <strong>speed</strong>. This is especially important when calling remote
functions. Nobody wants to wait 10 seconds just to finish a single calculation
made by changing a single parameter. That’s why I&rsquo;ve been contemplating
seriously building my own finite element solver instead of relying on my usual
tools.</p>
<p>It was here that Julia re-entered my radar. Originally thinking to just stay in
my comfort zone with Python and its critically acclaimed numerical package,
NumPy, I decided to test it out against Julia. Although that wasn’t the main
reason for me to see Julia again—it was actually Julia’s threading and
distributed computing capability—but we’ll talk about that in another post.</p>
<p>After all, NumPy is already well-known amongst the scientific computation
community as having one of the easiest to use, and fastest matrix solver out
there. Why not benchmark it against Julia, who’s been claimed to be <a href="https://www.matecdev.com/posts/numpy-julia-fortran.html">as fast
Fortran, with ease of use as
Python</a>?</p>
<h2 id="diving-to-the-code">Diving to the Code<a hidden class="anchor" aria-hidden="true" href="#diving-to-the-code">#</a></h2>
<h3 id="matrix-generation">Matrix Generation<a hidden class="anchor" aria-hidden="true" href="#matrix-generation">#</a></h3>
<p>The first thing I did was preparing the structure. I was thinking that the
project needs at least three files: one code file to generate the matrices, one
file to benchmark NumPy’s matrix solver, and one file to benchmark Julia’s
matrix solver.</p>
<p>Thus, I prepared a simple project structure like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>├── julia_side.jl
</span></span><span style="display:flex;"><span>├── matrix_generation.py
</span></span><span style="display:flex;"><span>└── numpy_side.py
</span></span></code></pre></div><p>I decided to use NumPy’s own matrix generation capability because, why not?</p>
<p>So firstly, the inside of the <code>matrix_generation.py</code> is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define matrix sizes</span>
</span></span><span style="display:flex;"><span>matrix_sizes <span style="color:#f92672">=</span> [<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1500</span>, <span style="color:#ae81ff">2000</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set the seed for reproducibility</span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the range for matrix elements</span>
</span></span><span style="display:flex;"><span>low_value <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>high_value <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> matrix_size <span style="color:#f92672">in</span> matrix_sizes:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Generate matrices</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(low_value, high_value, (matrix_size, matrix_size))
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(low_value, high_value, matrix_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculate B such that Ax = B</span>
</span></span><span style="display:flex;"><span>    B <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(A, x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Save matrices with size info</span>
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>savetxt(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;matrix_A_</span><span style="color:#e6db74">{</span>matrix_size<span style="color:#e6db74">}</span><span style="color:#e6db74">.txt&#34;</span>, A)
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>savetxt(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;vector_B_</span><span style="color:#e6db74">{</span>matrix_size<span style="color:#e6db74">}</span><span style="color:#e6db74">.txt&#34;</span>, B)
</span></span></code></pre></div><p>As you can see, I created six A matrices and six B matrices with varying
dimensions. To make sure that a matrix <strong>x</strong> solution exists, I generated <strong>x</strong>
first and then generated matrices B through the dot operation of A and <em>x</em>.
Then, I saved each of those matrices in simple text files.</p>
<p>Oh, also, I set the seed in NumPy, to make sure that the whole thing is
reproducible.</p>
<h3 id="benchmark-numpy">Benchmark NumPy<a hidden class="anchor" aria-hidden="true" href="#benchmark-numpy">#</a></h3>
<p>Next is writing a code to do benchmark of NumPy’s matrix solver. To do this, I
simply use two modules: <code>memory_profiler</code> and <code>timeit</code> to measure both the
execution time and memory usage.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> memory_profiler <span style="color:#f92672">import</span> memory_usage
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> timeit
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> gc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>matrix_sizes <span style="color:#f92672">=</span> [<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1500</span>, <span style="color:#ae81ff">2000</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">solve_equation</span>(A, B):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>solve(A, B)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> matrix_size <span style="color:#f92672">in</span> matrix_sizes:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Load matrices</span>
</span></span><span style="display:flex;"><span>    matrix_A <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;matrix_A_</span><span style="color:#e6db74">{</span>matrix_size<span style="color:#e6db74">}</span><span style="color:#e6db74">.txt&#34;</span>)
</span></span><span style="display:flex;"><span>    vector_B <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;vector_B_</span><span style="color:#e6db74">{</span>matrix_size<span style="color:#e6db74">}</span><span style="color:#e6db74">.txt&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Time the function using timeit</span>
</span></span><span style="display:flex;"><span>    elapsed_time <span style="color:#f92672">=</span> timeit<span style="color:#f92672">.</span>timeit(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;solve_equation(matrix_A, vector_B)&#34;</span>,
</span></span><span style="display:flex;"><span>        setup<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;from __main__ import solve_equation, matrix_A, vector_B&#34;</span>,
</span></span><span style="display:flex;"><span>        number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,  <span style="color:#75715e"># Number of executions</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Measure memory</span>
</span></span><span style="display:flex;"><span>    mem_usage_before <span style="color:#f92672">=</span> memory_usage(max_usage<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    mem_usage, vector_x <span style="color:#f92672">=</span> memory_usage(
</span></span><span style="display:flex;"><span>        (solve_equation, (matrix_A, vector_B)), max_usage<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, retval<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    mem_usage_increment <span style="color:#f92672">=</span> mem_usage <span style="color:#f92672">-</span> mem_usage_before
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Append results</span>
</span></span><span style="display:flex;"><span>    results<span style="color:#f92672">.</span>append((matrix_size, elapsed_time, mem_usage_increment))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Cleanup</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">del</span> matrix_A, vector_B, vector_x
</span></span><span style="display:flex;"><span>    gc<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save results to CSV</span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>savetxt(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;numpy_results.csv&#34;</span>,
</span></span><span style="display:flex;"><span>    results,
</span></span><span style="display:flex;"><span>    delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;,&#34;</span>,
</span></span><span style="display:flex;"><span>    header<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MatrixSize,Time,Memory&#34;</span>,
</span></span><span style="display:flex;"><span>    comments<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    fmt<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%.6f</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%.6f</span><span style="color:#e6db74">&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The results are then saved as a CSV file.This is important because I want to be
able to compare the data easily later.</p>
<h3 id="benchmark-julia">Benchmark Julia<a hidden class="anchor" aria-hidden="true" href="#benchmark-julia">#</a></h3>
<p>Next step is writing a Julia code to benchmark its solver. In Julia language,
there’s a package called <code>BenchmarkTools</code> which is very practical for measuring
the memory and time processing at the same time.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#66d9ef">using</span> BenchmarkTools, CSV, DataFrames, DelimitedFiles
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>matrix_sizes <span style="color:#f92672">=</span> [<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1500</span>, <span style="color:#ae81ff">2000</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">function</span> solve_equation(A, B)
</span></span><span style="display:flex;"><span>	x <span style="color:#f92672">=</span> A <span style="color:#f92672">\\</span> B
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> matrix_size <span style="color:#66d9ef">in</span> matrix_sizes
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># Use let block to limit the scope of variables and help with memory management</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">let</span> matrix_A <span style="color:#f92672">=</span> <span style="color:#66d9ef">Matrix</span>(CSV<span style="color:#f92672">.</span>File(<span style="color:#e6db74">&#34;matrix_A_</span><span style="color:#e6db74">$</span>(matrix_size)<span style="color:#e6db74">.txt&#34;</span>; header <span style="color:#f92672">=</span> false) <span style="color:#f92672">|&gt;</span> DataFrame),
</span></span><span style="display:flex;"><span>		df_B <span style="color:#f92672">=</span> CSV<span style="color:#f92672">.</span>File(<span style="color:#e6db74">&#34;vector_B_</span><span style="color:#e6db74">$</span>(matrix_size)<span style="color:#e6db74">.txt&#34;</span>; header <span style="color:#f92672">=</span> false) <span style="color:#f92672">|&gt;</span> DataFrame,
</span></span><span style="display:flex;"><span>		vector_B <span style="color:#f92672">=</span> df_B[<span style="color:#f92672">!</span>, <span style="color:#ae81ff">1</span>]  <span style="color:#75715e"># Extract the first column without copying</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># Run the function and benchmark</span>
</span></span><span style="display:flex;"><span>		bm <span style="color:#f92672">=</span> <span style="color:#a6e22e">@benchmark</span> solve_equation(<span style="color:#f92672">$</span>matrix_A, <span style="color:#f92672">$</span>vector_B)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		push!(results, (matrix_size, mean(bm)<span style="color:#f92672">.</span>time <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e9</span>, mean(bm)<span style="color:#f92672">.</span>memory <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e6</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># Explicitly call the garbage collector</span>
</span></span><span style="display:flex;"><span>		GC<span style="color:#f92672">.</span>gc()
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save results to CSV</span>
</span></span><span style="display:flex;"><span>open(<span style="color:#e6db74">&#34;julia_results.csv&#34;</span>, <span style="color:#e6db74">&#34;w&#34;</span>) <span style="color:#66d9ef">do</span> io println(io, <span style="color:#e6db74">&#34;MatrixSize,Time,Memory&#34;</span>)  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>	Adding a header line writedlm(io, results, <span style="color:#e6db74">&#39;,&#39;</span>)  <span style="color:#75715e"># Writing data below the header </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><p>Additionally, I also use <code>DataFrame</code> and <code>CSV</code> packages for help in reading the
matrices which are saved as text files.</p>
<p>Then, just like the benchmark code using NumPy, I also export the result as CSV
files.</p>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>I ran the tests using my own machine, whose specs can be summed as:</p>
<ul>
<li>Fedora Linux 38 64-bit</li>
<li>16 GiB System Memory</li>
<li>11th Gen Intel® Core™ i5-11400H × 12 Processors</li>
</ul>
<p>And below is the table with the results:</p>
<table>
<thead>
<tr>
<th>MatrixSize</th>
<th>NumPyTime</th>
<th>NumPyMemory</th>
<th>JuliaTime</th>
<th>JuliaMemory</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>7.60e-4</td>
<td>0.250</td>
<td>1.12e-6</td>
<td>0.001</td>
</tr>
<tr>
<td>100</td>
<td>3.95e-4</td>
<td>0.125</td>
<td>1.52e-4</td>
<td>0.082</td>
</tr>
<tr>
<td>500</td>
<td>4.05e-3</td>
<td>1.500</td>
<td>2.17e-3</td>
<td>2.008</td>
</tr>
<tr>
<td>1000</td>
<td>2.51e-2</td>
<td>4.625</td>
<td>7.79e-3</td>
<td>8.016</td>
</tr>
<tr>
<td>1500</td>
<td>2.61e-2</td>
<td>9.500</td>
<td>1.90e-2</td>
<td>18.024</td>
</tr>
<tr>
<td>2000</td>
<td>9.78e-2</td>
<td>13.375</td>
<td>3.86e-2</td>
<td>32.032</td>
</tr>
</tbody>
</table>
<p>From the data, it can be observed that Julia generally performs faster than
NumPy for solving matrices. In smaller matrices, the difference can be between
two or three times higher; however, with the increase of matrix dimensions, the
time difference became lower and lower. This makes me curious if the time
difference will at some point disappear. I’m going to study this part later.</p>
<p>Curiously, though, NumPy tends to use less memory compared to Julia. The
performance difference becomes more prominent as the size of the matrices
increases. We’re seeing about two to almost three times differences at the end.
This could also be a big consideration point.</p>
<h2 id="next-steps">Next Steps<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h2>
<p>The benchmark tests above have shown that Julia consistently performs faster
than NumPy for solving matrices. The next steps of the tests or decisions to
make, in my opinion, would be:</p>
<ul>
<li>Test for even bigger matrices (to-do next: 3000, 4000, 5000,…. the list goes
on).</li>
<li>Try other ways to input the data, such as REST API calls, instead of reading
from a text file.</li>
<li>Test in real-time use, such as setting them up as serverless functions to be
called or using REST API calls with workers and queue management.</li>
</ul>
<p>The whole code used for the benchmark test above has been uploaded to a <a href="https://github.com/ahmad-alkadri/curious-julia-numpy-matrix-solver">GitHub
repository.</a>
You&rsquo;re free to test it out, adjust, and modify. I&rsquo;ll probably also add more test
cases (especially for bigger matrices) in the near future, so stay in touch!</p>
<p>Also, don&rsquo;t hesitate to raise any questions or comments below!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>

  <div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
      
      
      if (window.location.hostname == "localhost")
                return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'ahmad-alkadri';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="/">Ahmad Alkadri</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
