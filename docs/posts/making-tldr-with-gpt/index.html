<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Making TL;DR app With GPT | Ahmad Alkadri</title>
<meta name="keywords" content="">
<meta name="description" content="Work&#39;s been quite pressing for the past couple of week; for relaxation, I sprint-coded a webapp (using Streamlit) to summarize articles on the web this weekend.">
<meta name="author" content="">
<link rel="canonical" href="/posts/making-tldr-with-gpt/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ce308c09b4317de5915a254af8d25c3549129148eb8584d37d76782e901c8f27.css" integrity="sha256-zjCMCbQxfeWRWiVK&#43;NJcNUkSkUjrhYTTfXZ4LpAcjyc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Making TL;DR app With GPT" />
<meta property="og:description" content="Work&#39;s been quite pressing for the past couple of week; for relaxation, I sprint-coded a webapp (using Streamlit) to summarize articles on the web this weekend." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/making-tldr-with-gpt/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-14T00:19:06+02:00" />
<meta property="article:modified_time" content="2023-05-14T00:19:06+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Making TL;DR app With GPT"/>
<meta name="twitter:description" content="Work&#39;s been quite pressing for the past couple of week; for relaxation, I sprint-coded a webapp (using Streamlit) to summarize articles on the web this weekend."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "/posts/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Making TL;DR app With GPT",
      "item": "/posts/making-tldr-with-gpt/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Making TL;DR app With GPT",
  "name": "Making TL;DR app With GPT",
  "description": "Work's been quite pressing for the past couple of week; for relaxation, I sprint-coded a webapp (using Streamlit) to summarize articles on the web this weekend.",
  "keywords": [
    
  ],
  "articleBody": "To tell you the truth, life has been a whirlwind of activity in recent weeks. Relentless pace of work, deadlines, bureaucracies, all of them combined left me feeling much more tired than usual, mentally and physically. But the big batch of calculations are finally finished; I just need now to finish the reports (no factor–gonna finish it in a single sitting on Monday), and this weekend I finally got some peace and spare time of my own.\nWhile browsing internet, I found some things that I thought interesting. Apparently, nowadays there are several twitter bots such as @LmaoGPT and @ReplyGPT that leverage the LLM (Large Language Model) to automatically create replies to tweets that mention them. You should check them out, by the way, they’re really interesting. Anyway, from them, I was inspired, and I thought to myself: why couldn’t I make something like that myself? Something that can leverage OpenAI’s GPT API?\nSo I started out like that, starting the python project, installing openai’s API, getting the API Key (I’ve been using ChatGPT for a while, and I’ve had the API for a while too for some work-related projects, but this is the first time I created one for my own project), testing them with some prompts, and then I ask myself: what should I make?\nI know that question should’ve been asked right at the very beginning (笑) before I even started the project. I browsed around a little bit more, opened up my email and caught up with some newsletter, found that some of them a bit too long to read and I better save it for later time, and that’s when it hit me: a TL;DR assistant! I could use ChatGPT for TL;DR-ing long articles, to make them shorter and easier to read. What’s more, with OpenAI API’s request format, I could even tell the GPT to generate summaries with certain characteristics: maximum or minimum length, tone of the text, and many more.\nChallenges Of course, not everything is fine and dandy. Very quickly, I found myself facing several problems.\nThe article retrieval itself is not a problem. A little beautifulsoup4 magic and it’s more or less done: make the request to the URL given by the user, extract its text, and save them as a string variable. Not difficult.\nThen I found myself facing the first challenge: the infamous token limit of OpenAI’s API. As explained here, my problem can be summed up quite simply by the following paragraph:\nDepending on the model used, requests can use up to 4097 tokens shared between prompt and completion. If your prompt is 4000 tokens, your completion can be 97 tokens at most.\nThis was actually quite a bigger problem than I expeted. Especially for something like an article summarizer, where we expect to send a prompt consisting of thousands of words, or tens of thousands of characters. Luckily for me, the hint for the solution to this problem is also written in the paragraph right after it:\n…there are often creative ways to solve problems within the limit, e.g. condensing your prompt, breaking the text into smaller pieces, etc.\nSo that’s what I did. With a pair of simple functions, I first divide the text from the URL into several chunks, and then I make the summary for each chunks before sending the summaries of all the chunks to the final request to the API that’ll make the final summary. This, of course, requires several request to the OpenAI API, but it solves the problem at least for now.\n(Hopefully, though, one day we wouldn’t have to do this again and we can send longer and longer text to the API endpoint in one go.)\nThe next challenge is about performances. As I said before, making summaries chunks by chunks will hinder the performance of the summarization process. We don’t want them to block each other, to wait each other to finish!\nLuckily, we have async-await in Python. By changing the whole preprocessing and processing functions into asynchronous, the chunks can be processed without blocking one another.\nFinally, there’s the problem of repeat summarization. Sending request to OpenAI’s API every single time a URL is submitted, even though the URL is the same, will quickly burn through the quota that I have. Thus, a caching system is important. I don’t want the caching to cause the same results every single time for the same URL though, so I was thinking of implementing it only for the summarization of chunks, not for the final one.\nThen came the question of how to implement it. I was thinking of using Streamlit’s native st.cache_data functionality, but I found out quickly that it’s not really compatible with async-await functionality in Python. Not only that, I also want the cache to expire after certain time (one hour sounds enough for me). Of course, there could be some tricks to make it work, but I want something simpler–something that can work out of the box without me having to reinvent the wheel.\nLuckily we have TTLCache from cachetools. It provides a quite high-level way to cache functions’ results and make it expire after certain interval of time has passed. So I implement it on the preprocessing part, and all is good!\nResults Just like my old twitter scraper project (it’s still not fixed following Twitter’s API update; I swear once I get enough spare time–probably two or three days–I’ll fix it), I decided to deploy the app on Streamlit Cloud. It works quite fast (even with me still using the Free Plan) and I’m quite happy with how it turned out. Some screenshots below:\nLanding page Summarization in-progress In no way or form do I profit from this post or has any commercial relations/interests/endorsements with Google at all Currently, it’s up and running and available on tldrgpt-web.streamlit.app. If you want to try it out, please go ahead! If you want to clone it and run it locally with your own OpenAI’s API Key, you’re also welcome! Found some bugs or problems? You’re welcome to open some Issues on the repo or comment here. I’ll always try to get back to you as soon as I can!\n",
  "wordCount" : "1033",
  "inLanguage": "en",
  "datePublished": "2023-05-14T00:19:06+02:00",
  "dateModified": "2023-05-14T00:19:06+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/posts/making-tldr-with-gpt/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ahmad Alkadri",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="Ahmad Alkadri (Alt + H)">Ahmad Alkadri</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="/page/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Making TL;DR app With GPT
    </h1>
    <div class="post-description">
      Work&#39;s been quite pressing for the past couple of week; for relaxation, I sprint-coded a webapp (using Streamlit) to summarize articles on the web this weekend.
    </div>
    <div class="post-meta"><span title='2023-05-14 00:19:06 +0200 CEST'>Sun, May 14, 2023</span>

</div>
  </header> 
  <div class="post-content"><p>To tell you the truth, life has been a whirlwind of activity in recent weeks. Relentless pace of work, deadlines, bureaucracies, all of them combined left me feeling much more tired than usual, mentally and physically. But the big batch of calculations are finally finished; I just need now to finish the reports (no factor&ndash;gonna finish it in a single sitting on Monday), and this weekend I finally got some peace and spare time of my own.</p>
<p>While browsing internet, I found some things that I thought interesting. Apparently, nowadays there are several twitter bots such as <a href="https://twitter.com/LmaoGPT">@LmaoGPT</a> and <a href="https://twitter.com/replygpt">@ReplyGPT</a> that leverage the LLM (Large Language Model) to automatically create replies to tweets that mention them. You should check them out, by the way, they&rsquo;re really interesting. Anyway, from them, I was inspired, and I thought to myself: why couldn&rsquo;t I make something like that myself? Something that can leverage OpenAI&rsquo;s GPT API?</p>
<p>So I started out like that, starting the python project, installing openai&rsquo;s API, getting the API Key (I&rsquo;ve been using ChatGPT for a while, and I&rsquo;ve had the API for a while too for some work-related projects, but this is the first time I created one for my own project), testing them with some prompts, and then I ask myself: what should I make?</p>
<p>I know that question should&rsquo;ve been asked right at the very beginning (笑) before I even started the project. I browsed around a little bit more, opened up my email and caught up with some newsletter, found that some of them a bit too long to read and I better save it for later time, and that&rsquo;s when it hit me: a TL;DR assistant! I could use ChatGPT for TL;DR-ing long articles, to make them shorter and easier to read. What&rsquo;s more, with OpenAI API&rsquo;s request format, I could even tell the GPT to generate summaries with certain characteristics: maximum or minimum length, tone of the text, and many more.</p>
<h2 id="challenges">Challenges<a hidden class="anchor" aria-hidden="true" href="#challenges">#</a></h2>
<p>Of course, not everything is fine and dandy. Very quickly, I found myself facing several problems.</p>
<p>The article retrieval itself is not a problem. A little <code>beautifulsoup4</code> magic and it&rsquo;s more or less done: make the request to the URL given by the user, extract its text, and save them as a string variable. <a href="https://github.com/ahmad-alkadri/tldrgpt-web/blob/main/src/textgetter.py">Not difficult</a>.</p>
<p>Then I found myself facing the first challenge: the infamous token limit of OpenAI&rsquo;s API. As explained <a href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them">here</a>, my problem can be summed up quite simply by the following paragraph:</p>
<blockquote>
<p>Depending on the model used, requests can use up to 4097 tokens shared between prompt and completion. If your prompt is 4000 tokens, your completion can be 97 tokens at most.</p>
</blockquote>
<p>This was actually quite a bigger problem than I expeted. Especially for something like an article summarizer, where we expect to send a prompt consisting of thousands of words, or tens of thousands of characters. Luckily for me, the hint for the solution to this problem is also written in the paragraph <em>right after</em> it:</p>
<blockquote>
<p>&hellip;there are often creative ways to solve problems within the limit, e.g. condensing your prompt, <strong>breaking the text into smaller pieces</strong>, etc.</p>
</blockquote>
<p>So that&rsquo;s what I did. With a pair of <a href="https://github.com/ahmad-alkadri/tldrgpt-web/blob/b013129c1708d22f383257f66a18ce44d646af6e/src/preprocess.py">simple functions</a>, I first divide the text from the URL into several chunks, and then I make the summary for each chunks before sending the summaries of all the chunks to the final request to the API that&rsquo;ll make the final summary. This, of course, requires several request to the OpenAI API, but it solves the problem at least for now.</p>
<p>(Hopefully, though, one day we wouldn&rsquo;t have to do this again and we can send longer and longer text to the API endpoint in one go.)</p>
<p>The next challenge is about <strong>performances</strong>. As I said before, making summaries chunks by chunks will hinder the performance of the summarization process. We don&rsquo;t want them to block each other, to wait each other to finish!</p>
<p>Luckily, we have async-await in Python. By changing the whole <a href="https://github.com/ahmad-alkadri/tldrgpt-web/blob/b013129c1708d22f383257f66a18ce44d646af6e/src/preprocess.py">preprocessing</a> and <a href="https://github.com/ahmad-alkadri/tldrgpt-web/blob/b013129c1708d22f383257f66a18ce44d646af6e/src/process.py">processing</a> functions into asynchronous, the chunks can be processed without blocking one another.</p>
<p>Finally, there&rsquo;s the problem of repeat summarization. Sending request to OpenAI&rsquo;s API every single time a URL is submitted, even though the URL is the same, will quickly burn through the quota that I have. Thus, a caching system is important. I don&rsquo;t want the caching to cause the same results every single time for the same URL though, so I was thinking of implementing it only for the summarization of chunks, not for the final one.</p>
<p>Then came the question of how to implement it. I was thinking of using Streamlit&rsquo;s native <code>st.cache_data</code> functionality, but I found out quickly that it&rsquo;s not really compatible with <code>async-await</code> functionality in Python. Not only that, I also want the cache to expire after certain time (one hour sounds enough for me). Of course, there could be some tricks to make it work, but I want something simpler&ndash;something that can work out of the box without me having to reinvent the wheel.</p>
<p>Luckily we have <code>TTLCache</code> from <code>cachetools</code>. It provides a quite high-level way to cache functions&rsquo; results and make it expire after certain interval of time has passed. So I implement it on the <a href="https://github.com/ahmad-alkadri/tldrgpt-web/blob/b013129c1708d22f383257f66a18ce44d646af6e/src/preprocess.py">preprocessing</a> part, and all is good!</p>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>Just like my old <a href="/posts/saying-goodbye-to-heroku">twitter scraper</a> project (it&rsquo;s still not fixed following Twitter&rsquo;s API update; I swear once I get enough spare time&ndash;probably two or three days&ndash;I&rsquo;ll fix it), I decided to deploy the app on Streamlit Cloud. It works quite fast (even with me still using the Free Plan) and I&rsquo;m quite happy with how it turned out. Some screenshots below:</p>
<table>
<thead>
<tr>
<th><img loading="lazy" src="/assets/img/making-tldr-with-gpt/ss_1.png" alt=""  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Landing page</em></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th><img loading="lazy" src="/assets/img/making-tldr-with-gpt/ss_2.png" alt=""  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Summarization in-progress</em></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th><img loading="lazy" src="/assets/img/making-tldr-with-gpt/ss_3.png" alt=""  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>In no way or form do I profit from this post or has any commercial relations/interests/endorsements with Google at all</em></td>
</tr>
</tbody>
</table>
<p>Currently, it&rsquo;s up and running and available on <a href="https://tldrgpt-web.streamlit.app">tldrgpt-web.streamlit.app</a>. If you want to try it out, please go ahead! If you want to clone it and run it locally with your own OpenAI&rsquo;s API Key, you&rsquo;re also welcome! Found some bugs or problems? You&rsquo;re welcome to open some Issues on the repo or comment here. I&rsquo;ll always try to get back to you as soon as I can!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>

  <div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
      
      
      if (window.location.hostname == "localhost")
                return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'ahmad-alkadri';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="/">Ahmad Alkadri</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
