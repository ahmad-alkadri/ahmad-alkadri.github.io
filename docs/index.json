[{"content":"A lot of things happened between my last post and now. Family stuff, work problems, dealing with bureaucracy—the list keeps growing.\nBut there are some good things too. One of them being that I finally beat my procrastination and finished Busuu\u0026rsquo;s courses for B1 level, and I even got the official certificate! You can check it out here. The kanji and listening parts were a bit tougher than I expected, lol.\nApart from that, I\u0026rsquo;ve been busy with a bunch of personal projects and experiments. One of them involves Julia—a high-level, dynamic language made for doing computation stuff. I first came across it during my PhD in France years ago.\nBack then, I was looking for a different language, framework, or platform to do numerical simulations. I\u0026rsquo;m talking about stuff like finite element, finite difference, and solving big matrices with tons and tons of elements over time (unsteady state). I needed something fast, easy to use and code, and easy for other people to use too.\nJulia was one of the options I found. I was amazed at how easy it was to install and set up. A couple of minutes and I was ready to go, writing my first functions.\n# hello.jl function say_hello() ## It looked more or less like this println(\u0026#34;Hello, world!\u0026#34;) end say_hello() The language seemed promising and performed well in tests, but I ultimately chose not to use it. Instead, I considered alternatives like MATLAB and Cast3M, which my supervisors were already familiar with.\nHowever, things have changed now.\nFrom Academia to Industry Lots of things happened, but basically I transitioned from academia to industry. Over the course of (almost) four years, I’ve gained more skills and experiences in software engineering than ever before. Building REST APIs, full-stack web apps, scrapers, and many more tools that are used on a daily basis at my company.\nThese days, though, when building a calculation tool, I came to realize a new requirement: speed. This is especially important when calling remote functions. Nobody wants to wait 10 seconds just to finish a single calculation made by changing a single parameter. That’s why I\u0026rsquo;ve been contemplating seriously building my own finite element solver instead of relying on my usual tools.\nIt was here that Julia re-entered my radar. Originally thinking to just stay in my comfort zone with Python and its critically acclaimed numerical package, NumPy, I decided to test it out against Julia. Although that wasn’t the main reason for me to see Julia again—it was actually Julia’s threading and distributed computing capability—but we’ll talk about that in another post.\nAfter all, NumPy is already well-known amongst the scientific computation community as having one of the easiest to use, and fastest matrix solver out there. Why not benchmark it against Julia, who’s been claimed to be as fast Fortran, with ease of use as Python?\nDiving to the Code Matrix Generation The first thing I did was preparing the structure. I was thinking that the project needs at least three files: one code file to generate the matrices, one file to benchmark NumPy’s matrix solver, and one file to benchmark Julia’s matrix solver.\nThus, I prepared a simple project structure like this:\n├── julia_side.jl ├── matrix_generation.py └── numpy_side.py I decided to use NumPy’s own matrix generation capability because, why not?\nSo firstly, the inside of the matrix_generation.py is as follows:\nimport numpy as np # Define matrix sizes matrix_sizes = [10, 100, 500, 1000, 1500, 2000] # Set the seed for reproducibility np.random.seed(0) # Define the range for matrix elements low_value = -10 high_value = 10 for matrix_size in matrix_sizes: # Generate matrices A = np.random.uniform(low_value, high_value, (matrix_size, matrix_size)) x = np.random.uniform(low_value, high_value, matrix_size) # Calculate B such that Ax = B B = np.dot(A, x) # Save matrices with size info np.savetxt(f\u0026#34;matrix_A_{matrix_size}.txt\u0026#34;, A) np.savetxt(f\u0026#34;vector_B_{matrix_size}.txt\u0026#34;, B) As you can see, I created six A matrices and six B matrices with varying dimensions. To make sure that a matrix x solution exists, I generated x first and then generated matrices B through the dot operation of A and x. Then, I saved each of those matrices in simple text files.\nOh, also, I set the seed in NumPy, to make sure that the whole thing is reproducible.\nBenchmark NumPy Next is writing a code to do benchmark of NumPy’s matrix solver. To do this, I simply use two modules: memory_profiler and timeit to measure both the execution time and memory usage.\nfrom memory_profiler import memory_usage import numpy as np import timeit import gc matrix_sizes = [10, 100, 500, 1000, 1500, 2000] def solve_equation(A, B): return np.linalg.solve(A, B) results = [] for matrix_size in matrix_sizes: # Load matrices matrix_A = np.loadtxt(f\u0026#34;matrix_A_{matrix_size}.txt\u0026#34;) vector_B = np.loadtxt(f\u0026#34;vector_B_{matrix_size}.txt\u0026#34;) # Time the function using timeit elapsed_time = timeit.timeit( \u0026#34;solve_equation(matrix_A, vector_B)\u0026#34;, setup=f\u0026#34;from __main__ import solve_equation, matrix_A, vector_B\u0026#34;, number=1, # Number of executions ) # Measure memory mem_usage_before = memory_usage(max_usage=True) mem_usage, vector_x = memory_usage( (solve_equation, (matrix_A, vector_B)), max_usage=True, retval=True ) mem_usage_increment = mem_usage - mem_usage_before # Append results results.append((matrix_size, elapsed_time, mem_usage_increment)) # Cleanup del matrix_A, vector_B, vector_x gc.collect() # Save results to CSV np.savetxt( \u0026#34;numpy_results.csv\u0026#34;, results, delimiter=\u0026#34;,\u0026#34;, header=\u0026#34;MatrixSize,Time,Memory\u0026#34;, comments=\u0026#34;\u0026#34;, fmt=[\u0026#34;%d\u0026#34;, \u0026#34;%.6f\u0026#34;, \u0026#34;%.6f\u0026#34;], ) The results are then saved as a CSV file.This is important because I want to be able to compare the data easily later.\nBenchmark Julia Next step is writing a Julia code to benchmark its solver. In Julia language, there’s a package called BenchmarkTools which is very practical for measuring the memory and time processing at the same time.\nusing BenchmarkTools, CSV, DataFrames, DelimitedFiles matrix_sizes = [10, 100, 500, 1000, 1500, 2000] results = [] function solve_equation(A, B) x = A \\\\ B end for matrix_size in matrix_sizes # Use let block to limit the scope of variables and help with memory management let matrix_A = Matrix(CSV.File(\u0026#34;matrix_A_$(matrix_size).txt\u0026#34;; header = false) |\u0026gt; DataFrame), df_B = CSV.File(\u0026#34;vector_B_$(matrix_size).txt\u0026#34;; header = false) |\u0026gt; DataFrame, vector_B = df_B[!, 1] # Extract the first column without copying # Run the function and benchmark bm = @benchmark solve_equation($matrix_A, $vector_B) push!(results, (matrix_size, mean(bm).time / 1e9, mean(bm).memory / 1e6)) # Explicitly call the garbage collector GC.gc() end end # Save results to CSV open(\u0026#34;julia_results.csv\u0026#34;, \u0026#34;w\u0026#34;) do io println(io, \u0026#34;MatrixSize,Time,Memory\u0026#34;) # Adding a header line writedlm(io, results, \u0026#39;,\u0026#39;) # Writing data below the header end Additionally, I also use DataFrame and CSV packages for help in reading the matrices which are saved as text files.\nThen, just like the benchmark code using NumPy, I also export the result as CSV files.\nResults I ran the tests using my own machine, whose specs can be summed as:\nFedora Linux 38 64-bit 16 GiB System Memory 11th Gen Intel® Core™ i5-11400H × 12 Processors And below is the table with the results:\nMatrixSize NumPyTime NumPyMemory JuliaTime JuliaMemory 10 7.60e-4 0.250 1.12e-6 0.001 100 3.95e-4 0.125 1.52e-4 0.082 500 4.05e-3 1.500 2.17e-3 2.008 1000 2.51e-2 4.625 7.79e-3 8.016 1500 2.61e-2 9.500 1.90e-2 18.024 2000 9.78e-2 13.375 3.86e-2 32.032 From the data, it can be observed that Julia generally performs faster than NumPy for solving matrices. In smaller matrices, the difference can be between two or three times higher; however, with the increase of matrix dimensions, the time difference became lower and lower. This makes me curious if the time difference will at some point disappear. I’m going to study this part later.\nCuriously, though, NumPy tends to use less memory compared to Julia. The performance difference becomes more prominent as the size of the matrices increases. We’re seeing about two to almost three times differences at the end. This could also be a big consideration point.\nNext Steps The benchmark tests above have shown that Julia consistently performs faster than NumPy for solving matrices. The next steps of the tests or decisions to make, in my opinion, would be:\nTest for even bigger matrices (to-do next: 3000, 4000, 5000,…. the list goes on). Try other ways to input the data, such as REST API calls, instead of reading from a text file. Test in real-time use, such as setting them up as serverless functions to be called or using REST API calls with workers and queue management. The whole code used for the benchmark test above has been uploaded to a GitHub repository. You\u0026rsquo;re free to test it out, adjust, and modify. I\u0026rsquo;ll probably also add more test cases (especially for bigger matrices) in the near future, so stay in touch!\nAlso, don\u0026rsquo;t hesitate to raise any questions or comments below!\n","permalink":"/posts/matrix-solving-with-julia-and-numpy-as-comparison/","summary":"A lot of things happened between my last post and now. Family stuff, work problems, dealing with bureaucracy—the list keeps growing.\nBut there are some good things too. One of them being that I finally beat my procrastination and finished Busuu\u0026rsquo;s courses for B1 level, and I even got the official certificate! You can check it out here. The kanji and listening parts were a bit tougher than I expected, lol.","title":"Matrix Solving with Julia (and Numpy as comparison)"},{"content":"I forgot where I first heard this, and if someone can tell me the source, I’d gladly cite them here. So here it goes:\nGiven the near-limitless size of the universe and the terribly small size of our capacity to save information, isn’t it more correct to say that, relative to the total knowledge available in the universe, our knowledge would always be closer to nothing than everything?\nThese words came to the forefront of my mind a week ago, after finishing The Wind-Up Bird Chronicle. They’ve stayed with me since, and I feel like writing them down to release some of the thoughts that have been bugging me.\nArtwork from the book that I bought, apparently part of Penguin Vintage series (I don’t know how it ended up at a small bookstore in Den Haag) Let me backtrack a little.\nThe Wind-Up Bird Chronicle is a book by Haruki Murakami, and it’s excellent. I read it once a long time ago and forgot most of it.\nThis happens to me often.\nMy memory isn’t great, especially lately with everything going on around me. And in my defence, when I said \u0026ldquo;a long time ago,\u0026rdquo; it was over a decade ago.\nDuring this summer vacation, I had the opportunity to visit a bookstore that sells English books with my family. My wife picked up a book titled \u0026ldquo;Antifragile\u0026rdquo; by Nassim Nicholas Taleb, while I chose a book by Haruki Murakami. Although I initially considered getting a collection of short stories by him, \u0026ldquo;The Wind-Up Bird Chronicle\u0026rdquo; caught my attention, so I decided to buy it.\nThe book itself was excellent, and I may do a full review someday. However, what particularly caught my attention was a chapter where the protagonist gets into a fight with his wife.\n\u0026ldquo;Why did you buy this stuff?\u0026rdquo; she asked, her voice weary.\nHolding the wok, I looked at her. Then I looked at the box of tissues and the package of toilet paper. I had no idea what she was trying to say.\n\u0026ldquo;What do you mean? They’re just tissues and toilet paper. We need those things. We’re not exactly out, but they won’t rot if they sit around a little while.\u0026rdquo;\n\u0026ldquo;No, of course not. But why did you have to buy blue tissues and flower-pattern toilet paper?\u0026rdquo;\n\u0026ldquo;I don’t get it,\u0026rdquo; I said, controlling myself. \u0026ldquo;They were on sale. Blue tissues are not going to turn your nose blue. What’s the big deal?\u0026rdquo;\n\u0026ldquo;It is a big deal. I hate blue tissues and flower-pattern toilet paper. Didn’t you know that?\u0026rdquo;\n\u0026ndash; Haruki Murakami, The Wind-Up Bird Chronicle\nAfter the fight, the protagonist made up with his wife (I’m simplifying here) and realised he didn’t know some things about her despite being married for years. He wondered if it was because he wasn’t attentive enough. He thought that if he had paid more attention, he would have known that his wife disliked blue tissues and flower-patterned toilet paper.\nBut even if he did pay attention to those things, he might still overlook other things about his wife. They might have another fight in the future about something like him not knowing that his wife hates a certain piece of furniture. Or vice versa. No one could be sure.\nThis led to his musings at the beginning of the chapter: is it possible for two people to fully understand each other?\nThis year marks the fifth anniversary of my wife and me living together under one roof and in one apartment room. We spend most of our time in close proximity to each other, except when we are at work. We can confidently say that we know each other quite well.\nHowever, we still sometimes surprise each other. For example, like I said above, when we were browsing the rows of books at the English bookstore, my wife picked up the book \u0026ldquo;Antifragile\u0026rdquo; by Nassim Nicholas Taleb.\nAt first, I wasn’t surprised, as I know she likes reading nonfiction and is into psychology and self-improvement books. But when I asked her why she picked it, she said, \u0026ldquo;I saw it mentioned in ‘Ikigai,’ another book that I read, and I’ve been interested in it ever since the concept was explained in that book.\u0026rdquo;\nThat surprised me. Although I know she has a book titled \u0026ldquo;Ikigai\u0026rdquo; at home and has read it, I don’t know what she read in it, and I certainly didn’t know she was interested in the concept of \u0026ldquo;Antifragile\u0026rdquo; to the point of wanting to buy a book about it.\nLike the main character in Murakami’s book, I realised that I don’t fully know my wife. A voice inside me said, \u0026ldquo;That’s only because you’ve been living with her for just five years. If it were longer, you would know her much better.\u0026rdquo; It’s true that I might know her better if we were together for longer, but I still wouldn’t know everything about her, would I?\nJust like me not knowing my siblings or parents fully, even though I basically lived with them for a very long time until my teenage years.\nIllustration by Harry Campbell on Antifragile Systems A critical part of my brain questions, \u0026ldquo;What do you mean by ‘knowing’? What do you mean by ‘fully knowing’?\u0026rdquo; Instead of engaging in a philosophical discussion or argument, I consulted dictionaries for answers, particularly regarding the first question.\nMerriam-Webster defines \u0026ldquo;knowing\u0026rdquo; as having or reflecting knowledge, information, or intelligence about something. Dictionary.com provides a similar definition, stating that \u0026ldquo;knowing\u0026rdquo; means having knowledge or information and being intelligent.\nThe common theme among these definitions is the possession of information. Therefore, to \u0026ldquo;know\u0026rdquo; something means to possess information about it.\nFully knowing something means possessing all information about it. However, is this even possible?\nIt’s probably still very difficult to imagine the state of \u0026ldquo;fully knowing\u0026rdquo;. For that I’d like to give an example with Python, a curious programming language that I like a lot.\nSo let’s say I want to create an object in Python, and make it so that other things in the program can know it fully. The processes mainly involve the following steps:\nCreate an object. Assign properties and attributes to that object. Allow others to check its properties to obtain the information regarding it or use it for other goals. Here is a simple example:\n# Define the object class Person: def __init__(self): self.height = 175 self.hair = \u0026#34;black\u0026#34; self.hobby = \u0026#34;coding\u0026#34; # Create a variable with that object ahmad = Person() # Check all of ahmad\u0026#39;s properties to fully know it: print(ahmad.height) print(ahmad.hair) print(ahmad.hobby) And that’s it. You and everything else in the running program now can know all the properties of a variable called ‘ahmad’. You can confidently say that you fully understand it.\nHowever, humans are not so easily defined.\nFor instance, how can we define a person and all of their characteristics?\nCertainly, we can ask them detailed questions to determine their traits and classify them into a particular MBTI type. Additionally, we can scan their entire body to determine the dimensions and surfaces of their body, as well as the inside of their body to determine the dimensions, shapes, and forms of their organs.\nIs that everything?\nNot quite, because how can we be sure to capture every single wrinkle on their face? The exact diameter of their nose, down to the smallest possible scale? The precise color shades of their eyes, down to the cell by cell level? And while we’re discussing cells, can we determine the forms, shapes, and positions of each and every cell in their body?\nLet’s set aside the physical measurements. How can we be certain that they honestly responded to the MBTI or personality test?\nWithout all of that, we can’t claim to know all of a person’s characteristics, and therefore, we can’t claim to know someone entirely.\nNever forget that no two people are exactly alike. Even the most identical of twins have unique fingerprints and different life experiences.\nIn a literal sense, everyone is unique.\nThis shouldn’t have surprised me as much as it does, but I have come to the realization that, in conclusion, we will never fully know or understand others.\nAnd forget about other people, we probably will never fully know ourselves neither.\nThus, should we abandon all hope of ever knowing someone fully?\nAnd if that’s the case, do we feel okay of simply live with someone, work with others, rely our livelihoods on people that we will never know fully?\nCertainly there should be some kind of threshold of knowing others. There must be some kind of \u0026ldquo;acceptable level\u0026rdquo; of knowing for a relationship to work between people.\nAnd maybe, once we reach that level, we would have certain level of faith with one another so that we can have peace with one another through that faith, hoping that that level is correct, sufficient, enough.\nLike Murakami said in his book:\nTo know one’s own state is not a simple matter. One cannot look directly at one’s own face with one’s own eyes, for example. One has no choice but to look at one’s reflection in the mirror. Through experience, we come to believe that the image is correct, but that is all.\n\u0026ndash; Haruki Murakami, The Wind-Up Bird Chronicle\n","permalink":"/posts/is-it-possible-to-really-know-someone/","summary":"I forgot where I first heard this, and if someone can tell me the source, I’d gladly cite them here. So here it goes:\nGiven the near-limitless size of the universe and the terribly small size of our capacity to save information, isn’t it more correct to say that, relative to the total knowledge available in the universe, our knowledge would always be closer to nothing than everything?\nThese words came to the forefront of my mind a week ago, after finishing The Wind-Up Bird Chronicle.","title":"Is It Possible to Really Know Someone?"},{"content":"Yesterday, someone informed me that my Tweet Cloud Maker hadn\u0026rsquo;t been working well for the entire morning until it finally stopped altogether \u0026ndash; it just returned warnings of empty results. At first, this was not surprising to me. I had just finished working on fixing and rewriting the backend tweet scraper a couple of weeks back, and I figured it would be just one of those bugs that I would need to fix.\nScreenshot of the app when I checked it After almost an hour of debugging and no result, I finally turned to my most trusted aide: Google. Almost immediately, I got the answer to my question: Twitter now requires an account to view tweets.\nScreenshot from the article After reading the article, I immediately turned to my next most trusted source of news: Twitter itself. Indeed, users have apparently noticed and have been complaining about the changes. Now, everyone has to be registered and logged in to view tweets, even from public Twitter users. But what about the Twitter API, especially v1.1?\nWell, I launched my Postman almost immediately to check, expecting at least some success. I assumed they would warn everyone before doing something as impactful as revoking one of the most crucial functionalities of their publicly available API, right? Something as disruptive as this?\nI was mistaken. The API v1.1 no longer works for scraping tweets.\nScreenshot of the API call to get the guest token Screenshot of the API call to get the tweets These are truly worldwide changes. They effectively end all Twitter scrapers, even those that have been in use for a long time by many people, like Nitter. And yes, the developers of Nitter have realized this too\u0026ndash;the repo has been alight with discussions.\nToday, Musk finally confirmed the changes that have occurred via Twitter. Well, sort of, because he didn\u0026rsquo;t directly address it; rather, he mentioned something that I think is even worse: all accounts on Twitter are now rate-limited. The newly unverified accounts can only read 300 tweets per day, old unverified accounts can read up to 600 tweets per day, and finally, verified accounts can read up to 6000 tweets per day.\nMind you, to get yourself verified on Twitter, you\u0026rsquo;ll have to pay. Musk is saying that these changes are being implemented to address \u0026ldquo;extreme levels of data scraping \u0026amp; system manipulation\u0026rdquo;, but honestly, I\u0026rsquo;m not buying it. Data scraping is practically unavoidable on the open internet; everyone from search engines to hobbyists (like myself) and even government agencies are doing it. There are ways to handle it, to prevent it from overburdening your system. IP limiters, device detection\u0026hellip; seriously, you can even implement it easily in your app with Flask-Limiter. I don\u0026rsquo;t see how a company as big as Twitter cannot implement one of these alternative solutions, anything other than what they\u0026rsquo;re doing right now\u0026hellip;\nAnyway, Musk has gone on to say that these changes are probably \u0026ldquo;temporary\u0026rdquo;. Well, I don\u0026rsquo;t know if they\u0026rsquo;re going to be temporary or not. I can\u0026rsquo;t really trust the words of \u0026ldquo;higher-ups\u0026rdquo; these days, but for now, I\u0026rsquo;ve decided to put Tweet Cloud Maker to sleep and put my work on pytterrator on pause. With Nitter also not functioning, I can only say:\nSo long, and thanks for all the tweets\n","permalink":"/posts/end-of-twitter-scraper/","summary":"Yesterday, someone informed me that my Tweet Cloud Maker hadn\u0026rsquo;t been working well for the entire morning until it finally stopped altogether \u0026ndash; it just returned warnings of empty results. At first, this was not surprising to me. I had just finished working on fixing and rewriting the backend tweet scraper a couple of weeks back, and I figured it would be just one of those bugs that I would need to fix.","title":"So Long, and Thanks For All The Tweets (Scraper)"},{"content":"Weekend, finally! Past weeks have been so intense both at work and home. It feels good being able to finally breathe a little bit. I hope everybody who\u0026rsquo;s reading this are having great time!\nSpeaking of great time, honestly, there used to be such time period in my life where weekends mean full days on the road, traveling, visiting quiet picturesque villages in Southern France with my wife. Or perhaps go to a bookstore to find some new Murakami books. Or maybe visit that café that looks so cozy, enjoying matcha or a bit of Boba tea.\nWell, those days are gone. Lately weekends are filled more with staying at home, resting ourselves after full week of work, watching TV, and maybe reading a little bit. Basically cozying up. Some friends told me it\u0026rsquo;s called \u0026ldquo;getting old\u0026rdquo;, and at this point, maybe I should be agreeing with him.\nNew Hobby and Tweet Cloud Maker On the other hand, I also picked up a new hobby since couple months ago. That is learning game development. It\u0026rsquo;s still a bit too early for me to show anything, I\u0026rsquo;ve been doing basically nothing except following a lot of tutorials on YouTube to get hands-on learning (note: if you\u0026rsquo;re learning Pygame and looking for some excellent tutorials, I would recommend this channel called Clear Code wholeheartedly. Hands down the best Pygame tutorials I\u0026rsquo;ve ever seen. Practically teaching me about game dev workflow from zero). Nevertheless, I really aim to create my own game one day\u0026ndash;maybe I should blog about it more later.\nAnyway, this blog shouldn\u0026rsquo;t be about my painful but so far satisfying process of learning game development. Not really. What happened was that about one or two weeks ago, while I was tidying up my github repo a little bit (so many abandoned weekend projects, gosh), I suddenly found my old Tweet Cloud Maker repo again. It was archived, and I remember that it was archived because, after some changes in Twitter API (I believe it was some time early this year), the snscrape module\u0026ndash;the primary module that I used to scrape tweets from Twitter\u0026ndash;stopped working for me.\nI remember myself not having too much time back then, and decided to take it down, promising myself to fix it one day.\nWell I unarchived the repo and started working on it again. Immediately, I still found myself having the primary module not working for me. I opened up their repo, browsing for their Issues and started reading, and basically got stopped at the following phrase on their README:\nIt is also possible to use snscrape as a library in Python, but this is currently undocumented.\nThat was kind of unfortunate. I was thinking of probably being a contributor to the repo, study it and write some documentations while trying to find out the issue for me, when I realized that it would take a bit more time and that it\u0026rsquo;d probably be faster for me to fix my old twitter scraping module and use it for this app.\nEnter the pytterrator Sometime around early last year, I started getting more conscious about privacy. I tried to make some efforts to stop using many apps that soak my data consistently. I stopped using most of my social media and finally deleting it one by one (privacy\u0026rsquo;s not the only reason, but it plays a big role in my decision; I\u0026rsquo;ll probably write a blog about it later). I don\u0026rsquo;t really like the idea of companies I don\u0026rsquo;t know using my data for profit without my full informed consent.\nI started looking for alternatives for using many of web services I\u0026rsquo;ve been using for a long time: Invidious for YouTube, SearxNG to replace Google, etc. One of the social media that I used consistently for searching up news around the world quickly is Twitter, and I stumbled upon Nitter, an alternative Twitter frontend that works with more or less the same principles as Invidious and SearxNG: get the data from the official site using API, strip away all the trackers, and display them in a cleaner, faster interface.\nI like it. It\u0026rsquo;s lightweight, I don\u0026rsquo;t have to install any applications to find out what Elon Musk is tweeting for example. I liked it a lot that I decided to start a weekend project back then called pytterrator that, I imagine, would become a simple Python module allowing us to get tweets using Twitter\u0026rsquo;s API and some scraping (if necessary), one that I\u0026rsquo;d be able to use as a foundation for making a full webapp just like Nitter.\nWhat Happened? Nothing. Long story short lots of things fell upon my lap in 2022 at work. I somehow got promoted. I got new responsibilities. The project gathered dust for months, and in the few moments I got to return to it, I couldn\u0026rsquo;t manage to do anything significant. Couple that with the changes in Twitter\u0026rsquo;s API, I thought to myself that I wouldn\u0026rsquo;t be able to fix and develop it further (or even taking care of my other side-projects) without having to sacrifice my career and sanity.\nThat kinda changed now. With the company expanding and developing, and more and more automated systems are put in place, I found my works to be much more streammlined and easy to manage lately. It feels good knowing that the company is slowly maturing, no longer a tiny startup comprising of five engineers working together in a single office. Everything started to fall into a more disciplined, consistent and well-structured routines, and I found myself having more time for my own projects (hence starting the game devs hobby lol).\nTo cut the story short: I opened up again the pytterrator repo and managed to make it work. It\u0026rsquo;s still so raw, no documentations yet put in place (I\u0026rsquo;ll definitely prepare it soon, promise), but it works to scrape tweets from public account. It uses Twitter v1.1 API, it puts some limiters to itself, it can regenerate guest token if needed. So far, it just works.\n(If you somehow trying it out and found some bugs on it please raise them as Issues or if you want you can help contribute directly! Would be really appreciated!)\nBack to Tweet Cloud Maker So that was four days ago. Then I came back to the more pressing problematic app of mine that is my Tweet Cloud Maker app. I decided to switch fully from the snscrape module to pytterrator.\nThe process wasn\u0026rsquo;t difficult at all. The scraper functions were simply removed and replaced by the methods defined in the pytterrator\u0026rsquo;s Client class. That alone managed to make the app up and running. It can scrape tweets up to certain number limit that we request (though I still put a limit of 1000 tweets\u0026ndash;this could potentially change in the future after I experiment and develop more on the module to handle the rate limit). The tweets are then digested, words containing special characters were removed.\nThen, as a bigger change, I simply remove the feature to scrape tweets between certain dates. The reason is simple: for now I haven\u0026rsquo;t implement the method to do this yet on pytterrator properly. Once I manage to do that on the module, I\u0026rsquo;ll simply update the module used in Tweet Cloud Maker and implement this new feature!\nResults Some screenshots of the app running:\nLanding page Scraping Elon Musk\u0026rsquo;s tweets There are still some works that I\u0026rsquo;d like to do though. I\u0026rsquo;d like for it to be able to recognize Chinese, Japanese, and Korean character for example. I\u0026rsquo;ve initiated this work on the app but it\u0026rsquo;s far from done. Unlike alphabetic sentences, words in Japanese or Chinese are seldom divided by spaces. Korean sentences do this but some parts still poorly recognized by the app. Stopwords or special characters are also need to be handled differently. This is definitely one of the stuffs that I\u0026rsquo;ll work on in the future, together with the date/calendar-based scraping.\nCurrently, it\u0026rsquo;s up and running and available on its old address at Streamlit Cloud: tweetcloudmaker.streamlit.app. If you want to try it out, please go ahead! If you want to clone it and run it locally on your machine, you\u0026rsquo;re also welcome! Found some bugs or problems? You\u0026rsquo;re welcome to open some Issues on the repo or comment here. I\u0026rsquo;ll try to get back to you as soon as I can!\nOnce again wishing everyone a nice weekend!\n","permalink":"/posts/fixing-twitter-scrapers/","summary":"Weekend, finally! Past weeks have been so intense both at work and home. It feels good being able to finally breathe a little bit. I hope everybody who\u0026rsquo;s reading this are having great time!\nSpeaking of great time, honestly, there used to be such time period in my life where weekends mean full days on the road, traveling, visiting quiet picturesque villages in Southern France with my wife. Or perhaps go to a bookstore to find some new Murakami books.","title":"Getting New Hobby and Fixing My Twitter Scrapers"},{"content":"To tell you the truth, life has been a whirlwind of activity in recent weeks. Relentless pace of work, deadlines, bureaucracies, all of them combined left me feeling much more tired than usual, mentally and physically. But the big batch of calculations are finally finished; I just need now to finish the reports (no factor\u0026ndash;gonna finish it in a single sitting on Monday), and this weekend I finally got some peace and spare time of my own.\nWhile browsing internet, I found some things that I thought interesting. Apparently, nowadays there are several twitter bots such as @LmaoGPT and @ReplyGPT that leverage the LLM (Large Language Model) to automatically create replies to tweets that mention them. You should check them out, by the way, they\u0026rsquo;re really interesting. Anyway, from them, I was inspired, and I thought to myself: why couldn\u0026rsquo;t I make something like that myself? Something that can leverage OpenAI\u0026rsquo;s GPT API?\nSo I started out like that, starting the python project, installing openai\u0026rsquo;s API, getting the API Key (I\u0026rsquo;ve been using ChatGPT for a while, and I\u0026rsquo;ve had the API for a while too for some work-related projects, but this is the first time I created one for my own project), testing them with some prompts, and then I ask myself: what should I make?\nI know that question should\u0026rsquo;ve been asked right at the very beginning (笑) before I even started the project. I browsed around a little bit more, opened up my email and caught up with some newsletter, found that some of them a bit too long to read and I better save it for later time, and that\u0026rsquo;s when it hit me: a TL;DR assistant! I could use ChatGPT for TL;DR-ing long articles, to make them shorter and easier to read. What\u0026rsquo;s more, with OpenAI API\u0026rsquo;s request format, I could even tell the GPT to generate summaries with certain characteristics: maximum or minimum length, tone of the text, and many more.\nChallenges Of course, not everything is fine and dandy. Very quickly, I found myself facing several problems.\nThe article retrieval itself is not a problem. A little beautifulsoup4 magic and it\u0026rsquo;s more or less done: make the request to the URL given by the user, extract its text, and save them as a string variable. Not difficult.\nThen I found myself facing the first challenge: the infamous token limit of OpenAI\u0026rsquo;s API. As explained here, my problem can be summed up quite simply by the following paragraph:\nDepending on the model used, requests can use up to 4097 tokens shared between prompt and completion. If your prompt is 4000 tokens, your completion can be 97 tokens at most.\nThis was actually quite a bigger problem than I expeted. Especially for something like an article summarizer, where we expect to send a prompt consisting of thousands of words, or tens of thousands of characters. Luckily for me, the hint for the solution to this problem is also written in the paragraph right after it:\n\u0026hellip;there are often creative ways to solve problems within the limit, e.g. condensing your prompt, breaking the text into smaller pieces, etc.\nSo that\u0026rsquo;s what I did. With a pair of simple functions, I first divide the text from the URL into several chunks, and then I make the summary for each chunks before sending the summaries of all the chunks to the final request to the API that\u0026rsquo;ll make the final summary. This, of course, requires several request to the OpenAI API, but it solves the problem at least for now.\n(Hopefully, though, one day we wouldn\u0026rsquo;t have to do this again and we can send longer and longer text to the API endpoint in one go.)\nThe next challenge is about performances. As I said before, making summaries chunks by chunks will hinder the performance of the summarization process. We don\u0026rsquo;t want them to block each other, to wait each other to finish!\nLuckily, we have async-await in Python. By changing the whole preprocessing and processing functions into asynchronous, the chunks can be processed without blocking one another.\nFinally, there\u0026rsquo;s the problem of repeat summarization. Sending request to OpenAI\u0026rsquo;s API every single time a URL is submitted, even though the URL is the same, will quickly burn through the quota that I have. Thus, a caching system is important. I don\u0026rsquo;t want the caching to cause the same results every single time for the same URL though, so I was thinking of implementing it only for the summarization of chunks, not for the final one.\nThen came the question of how to implement it. I was thinking of using Streamlit\u0026rsquo;s native st.cache_data functionality, but I found out quickly that it\u0026rsquo;s not really compatible with async-await functionality in Python. Not only that, I also want the cache to expire after certain time (one hour sounds enough for me). Of course, there could be some tricks to make it work, but I want something simpler\u0026ndash;something that can work out of the box without me having to reinvent the wheel.\nLuckily we have TTLCache from cachetools. It provides a quite high-level way to cache functions\u0026rsquo; results and make it expire after certain interval of time has passed. So I implement it on the preprocessing part, and all is good!\nResults Just like my old twitter scraper project (it\u0026rsquo;s still not fixed following Twitter\u0026rsquo;s API update; I swear once I get enough spare time\u0026ndash;probably two or three days\u0026ndash;I\u0026rsquo;ll fix it), I decided to deploy the app on Streamlit Cloud. It works quite fast (even with me still using the Free Plan) and I\u0026rsquo;m quite happy with how it turned out. Some screenshots below:\nLanding page Summarization in-progress In no way or form do I profit from this post or has any commercial relations/interests/endorsements with Google at all Currently, it\u0026rsquo;s up and running and available on tldrgpt-web.streamlit.app. If you want to try it out, please go ahead! If you want to clone it and run it locally with your own OpenAI\u0026rsquo;s API Key, you\u0026rsquo;re also welcome! Found some bugs or problems? You\u0026rsquo;re welcome to open some Issues on the repo or comment here. I\u0026rsquo;ll always try to get back to you as soon as I can!\n","permalink":"/posts/making-tldr-with-gpt/","summary":"To tell you the truth, life has been a whirlwind of activity in recent weeks. Relentless pace of work, deadlines, bureaucracies, all of them combined left me feeling much more tired than usual, mentally and physically. But the big batch of calculations are finally finished; I just need now to finish the reports (no factor\u0026ndash;gonna finish it in a single sitting on Monday), and this weekend I finally got some peace and spare time of my own.","title":"Making TL;DR app With GPT"},{"content":"Adulthood really has its way of taking up so much of our waking time, doesn’t it? It feels like it was only yesterday I was upset by the end of Heroku’s free plan. A blink of an eye and suddenly we’re closer to the end of Spring and the beginning of Summer than we are to the snowy days in Luxembourg.\nTime flies.\nWhat happened in the meantime for me was a lot. Work stuff, of course, as usual, doing some design and engineering stuff. On the other hand, there are also my recent adventures in learning new things. A couple of those stood out for me: the first is learning Rust using ChatGPT, and the second is my experience with learning Japanese through mobile apps.\nFirstly, let’s talk about ChatGPT. I think by now everybody knows what ChatGPT is. Especially with the latest release of its model, in which it has made some great achievements. I’ll admit that I’m probably among the early users of it, for a lot of reasons, but mostly for helping me code. Sure, some people say that it’s terrible for coding, mostly because there are some mismatches between the versioning of the modules it’s using, and bizarrely it also put out some methods or packages that don’t exist, but I think one of its strongest strengths (at least for me) is for teaching new languages.\nAnd that’s basically the majority of my uses of it. Since December last year, I’ve been using this AI-powered language model to learn several programming languages. One of them, the one I’m focusing on most right now, is Rust.\nRust, as many of you may already know, is a systems programming language that focuses on safety, concurrency, and performance. One of its core features is the ownership system, which ensures memory safety without a garbage collector. This, in turn, leads to more predictable performance and efficient memory management. For example, consider the following code snippet:\nfn main() { let s1 = String::from(\u0026#34;Hello, world!\u0026#34;); let s2 = s1; println!(\u0026#34;{}\u0026#34;, s1); } In this example, Rust’s ownership system prevents the use of s1 after transferring ownership to s2. This eliminates the risk of a double-free error, ensuring memory safety.\nAnother positive aspect of Rust is its strong community support, which has led to the creation of numerous libraries and frameworks, such as Actix for web development or Tokio for asynchronous runtime. Let me tell you this though; I love how Rust handles its async functionality. Having been working with async-await on mostly JavaScript (Node.js), I’ve been getting such a fresh feeling from Rust. Especially with its great compiler, deploying web apps (backend, APIs, etc.) is really a breeze.\nTruly, maybe one day I should write a full blog about the whole async functionalities in Rust. It’s going to take more than one post, though, of that I’m sure.\nOn a different note, I’ve been learning Japanese using mobile apps. I started out with only Duolingo, but these days I’m also advancing with Busuu. I like Duolingo’s casualness and more “fun” vibe, but Busuu’s structure has been particularly rewarding, especially in my efforts to learn the grammar. Really, Japanese is surprisingly a grammar-heavy language, with strong underlining and differences between casual vs formal tones and words.\nAlso, surprisingly, while kanji is very intimidating, after a while, once you get the hang of basic kanji characters, they’re actually super helpful in understanding sentences and phrases way faster. It’s like having a shortcut when you’re reading, without needing to figure out every single hiragana or katakana character.\nTake this sentence: “わたしはほんをよみます” (Watashi wa hon wo yomimasu). Now check it out with kanji: “私は本を読みます.” See how the kanji characters 私 (watashi: I), 本 (hon: book), and 読 (yomi: read) give you the key info you need to understand that the sentence means “I read a book”. Pretty neat!\nHere’s another longer one: “かのじょはとうきょうにすんでいます” (Kanojo wa Toukyou ni sunde imasu). With kanji, it becomes “彼女は東京に住んでいます.” The kanji characters 彼女 (kanojo: she), 東京 (Toukyou), and 住 (sumu: lives) help you figure out that the sentence is saying “She lives in Tokyo.” Super handy.\nHowever, while these apps–Busuu, Duolingo, and others–offer a convenient and efficient way to learn the language (I literally learn mostly when on the bus haha), the lack of a conversation partner has proven to be a significant disadvantage. It’s one thing being able to understand texts from manga and novels, or even understanding what is being sung or said in J-Pop songs or J-Drama films, but it’s fully different once I tried to express myself in Japanese. Especially when it starts being a back-and-forth conversation.\nFor that, I think I definitely need some people I can speak and interact with, every day.\nI remember eight, seven years ago, when I arrived in France for my Master’s. Even with B1 certification; ultimately it was by immersing myself in the language and using it daily with native speakers that made me become really fluent. This is why these days I’ve been trying to find a Japanese learning community with whom I can practice and grow together.\nIn conclusion, the past few months have been filled with challenges and growth as I’ve dived into the world of Rust and Japanese. Utilizing tools like ChatGPT and mobile apps has given me the opportunity to expand my skill set, but the importance of direct human interaction in learning is still unmatchable. Going forward I think I’ll start sharing more notes from my efforts in learning, be it Rust or other programming languages or frameworks or Japanese.\n","permalink":"/posts/learning-notes-rust-japanese/","summary":"Adulthood really has its way of taking up so much of our waking time, doesn’t it? It feels like it was only yesterday I was upset by the end of Heroku’s free plan. A blink of an eye and suddenly we’re closer to the end of Spring and the beginning of Summer than we are to the snowy days in Luxembourg.\nTime flies.\nWhat happened in the meantime for me was a lot.","title":"Code and Kanji: Small Notes from Learning Rust and Japanese"},{"content":"Why the return? Well I already have my own website at Wordpress here, but seeing the changes in its premium plan, and its overly complex interface (I just need something simple to write some post, afterall), I thus decided to try to return writing here. Afterall, I\u0026rsquo;m not looking to share anything more than code snippets and learning notes on my blog from now on. There are many reasons for this, and one day I\u0026rsquo;ll probably write them up fully here.\nFor now, I\u0026rsquo;ll just continue working on this site. See you soon!\n","permalink":"/posts/finally-redeployed/","summary":"Why the return? Well I already have my own website at Wordpress here, but seeing the changes in its premium plan, and its overly complex interface (I just need something simple to write some post, afterall), I thus decided to try to return writing here. Afterall, I\u0026rsquo;m not looking to share anything more than code snippets and learning notes on my blog from now on. There are many reasons for this, and one day I\u0026rsquo;ll probably write them up fully here.","title":"Finally Redeployed"},{"content":" So long, and thanks for the free Heroku!\n\u0026ndash; not Douglas Adams (maybe)\nI was thinking of writing this post months ago, ever since the infamous post by Heroku announcing the end of their free-tier plan, but a severe combination of the workplace’s busyness and homebound procrastination between September and November prevented me from doing so. Of course, I could blame nobody because of it. Now, with me finally returning from my vacation (gonna dedicate a full blog post about it later I think), and things cooling down at work (it actually isn’t, but things are much more ordered now with the arrival of new engineers–again, gonna dedicate a post for it later), I finally got the time to end my Heroku apps, migrate most of them, and of course, write this post.\nFirst thing first: I’ve never been a formally trained developer. People at work–colleagues, clients–asked this from time to time, wondering what on earth a guy with a Bachelor of Forestry doing in one of Luxembourg’s biggest, fastest-growing startups as a Senior Simulation Engineer.\nLong story short: during my Master’s, specifically, during my internship, realizing that it would take forever to digest the experimental data using Microsoft Excel, I started learning to code in R. It was quite big for me because even though I knew some coding (MATLAB, specifically), it was my first foray into a real, legit open-source programming language. To say it was life-changing was an understatement. Within the span of a couple of months, I learned how to automatize data input, storage, query, analysis, and reporting. Not only that, I managed to build simple statistical models that could predict the acoustic properties of wood from its anatomical characteristics. It got published in a scientific journal. Life was good.\nHowever, if R opened my eyes to the world of automatization, data science, data engineering, and to a certain degree, machine learning, it was Python that brought me fully into the software development realm. After being introduced to it during my Ph.D., I learned how to build apps for the first time. Mostly command line apps, they helped me to check on my experiments, store and query data, analyze, build and rebuild models, simulate physical phenomena, visualize them, and many others. Then I learned how to package those command line apps into one big app with many endpoints–basically building REST APIs–and I learned how to deploy them.\nAnd this was where Heroku came.\nDeploying a web app in Heroku is so easy. It felt, in the beginning, like some cheat codes that people have been hiding from me all this time. I spent so much time learning about server, web server, firewall-ing, and dockerization, and you tell me that I can just heroku push and my app would be deployed? And then I could share and use the link with anyone, from anywhere, in the world? For free?? (important part, because as a student, back then, well, money was tight–and probably the same case for many people as well)\nOf course, there are a lot of limitations to using free dynos. The app must be awakened first before being used. There are limits to the free hours it can be used. Limitation in memories also. All those things need to be taken into account. Production-wise, it doesn’t make sense to deploy your app to Heroku’s free-tier dynos.\nOn the other hand, development-wise, it makes so much sense to use free-tier. And, for someone like me, who literally learned how to code and build apps from scratch through people’s blog posts, YouTube, and free ebooks, Heroku feels like a “safe space” where I can test things I’ve learned. I came to like Heroku so much, that I promoted it to be used for production deployments of some apps that I participated in developing at my workplaces.\nYesterday, at last, I managed to migrate all of the apps that I’ve deployed at Heroku and dockerized them, and put them, for now, on my personal server at home. Thanks to the projects that I’ve done at work, dockerizing–especially for Flask apps, the framework I’ve been studying and using the most–feels like breathing for me these days. Some Streamlit apps that I’ve made as a hobby, for example, the tweet cloud maker, have been redeployed at Streamlit Cloud. You can even use it now–it feels faster than before when it was hosted on Heroku!\nI’m currently looking for alternatives to where to deploy the other apps. There are many of them, of course, and one day perhaps I could list them in a separate, dedicated post. Though, to be honest, now having them on my own, personal server, this could probably become a path for me to learn how to do proper deployment on my own. The security part would certainly be the hardest to overcome, but seeing that most of those apps are just, well, hobby apps, I don’t feel there is any rush.\nFinally, as so many people have said–even though I mostly use the free-tier Heroku to study, learn, test new things, and share apps with others (mostly friends and close colleagues), its departure really feels like the end of an era. Heroku free-tier has accompanied me throughout the learning phase of my coding life, and I’m very thankful for it. Here’s hoping the next generation of developers–especially the self-taught ones like me–can find a good alternative for their development path!\n","permalink":"/posts/saying-goodbye-to-heroku/","summary":"So long, and thanks for the free Heroku!\n\u0026ndash; not Douglas Adams (maybe)\nI was thinking of writing this post months ago, ever since the infamous post by Heroku announcing the end of their free-tier plan, but a severe combination of the workplace’s busyness and homebound procrastination between September and November prevented me from doing so. Of course, I could blame nobody because of it. Now, with me finally returning from my vacation (gonna dedicate a full blog post about it later I think), and things cooling down at work (it actually isn’t, but things are much more ordered now with the arrival of new engineers–again, gonna dedicate a post for it later), I finally got the time to end my Heroku apps, migrate most of them, and of course, write this post.","title":"Saying Goodbye to Heroku"},{"content":"My name is Ahmad Alkadri. You can find out more about me profesionally on my LinkedIn. Sometimes I travel and took photos.\nI came to Europe in 2015, specifically to France, for my Master\u0026rsquo;s study. Originally, I came from the field of Forestry. Due to some weird course of fate, I became entangled in the field of wood acoustics, wrote a scientific article about violin wood, and worked as an Applied Researcher in the R\u0026amp;D department of Henri SELMER Paris–one of the best clarinet manufacturer in the world.\nToday, I work as a Senior Simulation Engineer at Leko Labs\nNowadays I work a lot more with Python for doing simulations, especially building construction simulations. I also wrote some ML codes and bots to automate some processes. In my spare time I read books, play music, or travel. I don’t know yet what will I do with this blog (maybe just blogging codes), but for now, I’ll just make it as another one of my little corners on the web.\n","permalink":"/page/about/","summary":"My name is Ahmad Alkadri. You can find out more about me profesionally on my LinkedIn. Sometimes I travel and took photos.\nI came to Europe in 2015, specifically to France, for my Master\u0026rsquo;s study. Originally, I came from the field of Forestry. Due to some weird course of fate, I became entangled in the field of wood acoustics, wrote a scientific article about violin wood, and worked as an Applied Researcher in the R\u0026amp;D department of Henri SELMER Paris–one of the best clarinet manufacturer in the world.","title":"About"}]